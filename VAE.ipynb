{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src=\"./data/model_architecturev2.png\" alt=\"Image\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explores and implements the ideas of the seminal deep learning paper \"Auto-Encoding Variational Bayes\". Link to the paper: [arxiv](https://arxiv.org/abs/1312.6114)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this paper important?\n",
    "\n",
    "The paper introduces the Variational Autoencoder (VAE), a powerful model for generative tasks, which involves generating new data samples similar to a given dataset. The paper develops mathematical solutions to the problem of using an autoencoder to generate new samples. Since autoencoders are generally trained in an unsupervised fashion—without relying on explicit data labels—they offer a versatile technique. This is why the can be adapted to a wide range of problems in deep learning. \n",
    "VAEs, on their own, are usually not state-of-the-art for specific tasks. However, their adaptability makes them a key building block in larger model architectures. For example, the current state of the art architecture for image generation - Stable Diffusion, relies on VAE as one of its core building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a VAE model is to learn a distribution of a latent hidden variable $z$ that explains the observed data $x$. There are two key challenges in this:\n",
    "1. Computing the posterior distribution $p(z|x)$, which is often intractable due to the complexity of the marginal likelihood of the data $p(x)$\n",
    "2. Efficiently learning model parameters through optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors propose using **variational inference**, a framework for approximating intractable posterior distributions, combined with an efficient reparameterization trick to enable gradient-based optimization. This approach allows for training both the generative model and the approximate posterior simultaneously. The resulting model, the VAE, combines probabilistic modeling with neural networks, making it scalable and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data that we observe can be thought of as being represented or generated by a hidden latent variable, which cannot be observed directly. An example of this concept can be seen in Plato's \"Allegory of the Cave.\" In Plato's work, a group of people are chained inside a cave and can only see shadows on the wall created by unseen objects passing before a fire. The shadows—a two-dimensional projection—are the data that can be observed, while the hidden three-dimensional objects are the real driving force behind the observations; they are the latent variable.\n",
    "\n",
    "This latent variable can be either of higher dimensionality, as in Plato's Cave, or of lower dimensionality than the observed data. However, in the field of generative modeling, we generally seek to learn lower-dimensional latent representations. Reducing the dimensionality of the data can be seen as a form of compression and also has the potential to lead to semantically meaningful structures in the latent space in relation to the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evidence Lower Bound - ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the observed data $x$ and the latent variable $z$ can be modeled by a joint distribution:\n",
    "$$p(x|z)$$\n",
    "In the case of generative modeling one approach is to learn a model to maximize the likelihood $P(x)$ for all observed data points $x$. This simply means that a trained model should assign high likelihood to real observed data, and low likelihood to everything else. To access the likelihood of the observed data $P(x)$ we could manipulate the joint distribution of observed data and latent variable $P(x|z)$ in two different ways:\n",
    "*  The first is to marginalize out the latent variable $z$, by integrating the      probability of the observed data across all possible values of $z$:\n",
    "    $$p(x) = \\int p(x, z)dz$$\n",
    "    The problem is that computing this integral is not possible for more complex models. Some of the reasons for this are potential high dimensionality of the latent space and the lack of closed form solutions of complex functions involving neural networks.\n",
    "\n",
    "* The second way to get $p(x)$ on its own is to use the chain rule of probability:\n",
    "  $$P(X_1, X_2, \\dots, X_n) = \\prod_{i=1}^n P(X_i \\mid X_1, X_2, \\dots, X_{i-1})$$\n",
    "\n",
    "    $$p(x,z) = p(x)p(z|x)$$ \n",
    "    dividing by $p(z|x)$ we get\n",
    "    $$p(x) = \\frac{p(x,z)}{p(z|x)}$$ \n",
    "\n",
    "    This has the problem that it involves having access to ground truth latent encoder $p(z|x)$, which is not available since the latent variable is by definition not directly observable and there is no ground truth mapping from $x$ to $z$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea to solve this problems is to use the two equations for $p(x)$ above to derive a term called Evidence Lower Bound(ELBO), which places a lower bound on the evidence- the log likelihood of the observed data. This allows us to define an objective for the latent variable model- to maximize the probability of the observed data indirectly, by maximizing the ELBO - the lower bound of the log of that probability. Below is the equation for the ELBO and its connection to evidence:\n",
    "$$ \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[ \\log \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)} \\right] $$\n",
    "$$$$\n",
    "$$\\log p(x) \\ge \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[ \\log \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)} \\right] $$\n",
    "$$$$\n",
    "where we introduce $q_{\\phi}(z \\mid x)$ as a flexible approximate variational distribution with parameters $\\phi$ instead of the original intractable distribution $p(z|x)$. The idea is that a model is learned to estimate the true intractable distribution of latent variables over given observations for $x$.\n",
    "As the models is trained its parameters are optimized and its estimation of the true prior distribution becomes better and better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELBO Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive the ELBO we start with the expression for marginalizing the latent variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x) = \\int p(x, z)dz$$\n",
    "apply log on both sides,\n",
    "$$ \\log p(x) = \\log \\int p(x, z)dz$$\n",
    "multiply the integrand by one in the form of $\\frac{q_{\\phi}(z \\mid x)}{q_{\\phi}(z \\mid x)}$ to introduce the approximate variational distribution(the estimation of the intractable posterior learnt by a neural network)\n",
    "$$ \\log p_{\\theta}(x) = \\log \\int q_{\\phi}(z \\mid x) \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)} \\, dz $$\n",
    "Now, the expectation of a arbitrary deterministic expression $f(x)$ is given by the integral $$ \\mathbb{E}[f(x)] = \\int f(x) p(x) \\, dx $$ therefore, the integral expression for the evidence can be rewritten as an expectation of the function $\\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)} $ under the distribution $ q_{\\phi}(z \\mid x) $:\n",
    "$$ \\log p(x) = \\log \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[ \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)} \\right] $$\n",
    "Now, because of the logarithm's concavity and the fact that the log of an expectation is always greater than or equal to the expectation of the log (this is a consequence of Jensen's inequality):$$ \\log \\mathbb{E}[f(z)] \\geq \\mathbb{E}[\\log f(z)] $$\n",
    "we can bring the log inside the expectation:\n",
    "$$ \\log p(x) \\ge  \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left  [ \\log \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)} \\right] $$\n",
    "\n",
    "Thus we have derived the lower bound of the evidence. Now we can explore the ELBO term further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELBO in More Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start with the equation of the ELBO:\n",
    "$$\\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left  [ \\log \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)} \\right]$$\n",
    "We can apply the chain rule of probability to substitute $p_{\\theta}(x, z)$ with $p_{\\theta}(x|z)p(z)$:\n",
    "$$\\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left  [ \\log \\frac{p_{\\theta}(x|z)p(z)}{q_{\\phi}(z \\mid x)} \\right]$$\n",
    "Next, since expectation is linear and using logarithm rules we can split the expression like this:\n",
    "$$\\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left  [ \\log p_{\\theta}(x|z) \\right] +  \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left  [ \\log \\frac{p_{}(z)}{q_{\\phi}(z \\mid x)} \\right] $$\n",
    "\n",
    "Here we can recognize the second term, it is Kullback–Leibler Divergence which provides a way measure if two distributions are close together or not. It can be though of as the distance between two distributions since like distance it is also always non-negative. There is a small caveat with this interpretation though, since unlike distance KL divergence is not symmetric. Namely the KL divergence between distributions A and B is not the same as B and A. The definition of KL divergence is:$$ D_{\\text{KL}}(P \\parallel Q) = \\mathbb{E}_{x \\sim P} \\left[ \\log \\frac{p(x)}{q(x)} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we substitute x in the general distribution with $z$ we get the second term of the previous expression:\n",
    "$$\n",
    "\\underset{\\text{Reconstruction term}}{\\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[ \\log p_{\\theta}(x \\mid z) \\right]} \n",
    "\n",
    "\\underset{\\text{KL Divergence term}}{- D_{\\text{KL}}(q_{\\phi}(z \\mid x) \\parallel p(z))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"./data/encoder_decoder.png\" alt=\"Image\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "We can gain a better intuition about this evidence expression if we look at the larger picture. The variational distribution $ q_{\\phi}(z \\mid x) $ is a neural network that is learned to approximate the hidden posterior distribution. This network is the encoder, it transforms inputs into distribution over possible latents. Similarly $p(x|z)$ is another network learnt to convert a given latent variable vector $z$ into an observation $x$. This second network is the decoder.<br>\n",
    "\n",
    "Each term in the ELBO expression above measures the performance of one of these two networks:\n",
    "* The reconstruction term measures the performance of the decoder, it ensures that the learned distribution is modeling effective latents that the original data can be generated from.\n",
    "* The second term measures the performance of the encoder, it ensures that the learned variational distribution is as similar as possible to the prior belief held over the latent variables.\n",
    "\n",
    "As already discussed, we want the trained model to maximize the probability of observed samples. Since, evidence is the log of that probability, maximizing the evidence lower bound accomplishes that goal. From the final expression of the ELBO it is evident that maximizing it is equivalent to maximizing the first (reconstruction) term and minimizing the second (KL Divergence) term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Reparameterizaion Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a major hurdle for training a model such as the one we outlined in previous paragraphs. Namely, to optimize the ELBO via backpropagation, we need gradients of the ELBO with respect to the encoder's parameters $\\phi$. However, we cannot compute these gradients. This is because  the Evidence Lower Bound (ELBO) contains an expectation over the approximate posterior distribution $q_{\\phi}(z \\mid x)$. This expectation involves sampling latent variables $z$ during training. However, the sampling operation introduces stochasticity in the computation, which disrupts the computational graph. Information about gradients enccounters a bottleneck and cannot flow through a random sampling process. To define the problem in more rigorously, if we want to take the gradient w.r.t. $\\theta$ of this expectation - $E_{p(z)} [f_{\\theta}(z)]$, we can rewrite the expectation as an integral,then bring the gradient inside the integral and rewrite it as an expectation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta} E_{p(z)} [f_{\\theta}(z)] \n",
    "&= \\nabla_{\\theta} \\left[ \\int_{z} p(z) f_{\\theta}(z) \\, dz \\right] \\\\\n",
    "&= \\int_{z} p(z) \\left[ \\nabla_{\\theta} f_{\\theta}(z) \\right] \\, dz \\\\\n",
    "&= E_{p(z)} \\left[ \\nabla_{\\theta} f_{\\theta}(z) \\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the gradient of the expectation is equal to the expectation of the gradient. However if we try to take the gradient with respect to $\\theta$ we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\theta} E_{p_{\\theta}(z)} [f_{\\theta}(z)] \n",
    "&= \\nabla_{\\theta} \\left[ \\int_{z} p_{\\theta}(z) f_{\\theta}(z) \\, dz \\right] \\\\\n",
    "&= \\int_{z} \\nabla_{\\theta} \\left[ p_{\\theta}(z) f_{\\theta}(z) \\right] \\, dz \\\\\n",
    "&= \\int_{z} f_{\\theta}(z) \\nabla_{\\theta} p_{\\theta}(z) \\, dz \n",
    "+ \\int_{z} p_{\\theta}(z) \\nabla_{\\theta} f_{\\theta}(z) \\, dz \\\\\n",
    "&= \\underset{\\text{(Problematic Term)}}{\\int_{z} f_{\\theta}(z) \\nabla_{\\theta} p_{\\theta}(z) \\, dz} \n",
    "+ E_{p_{\\theta}(z)} \\left[ \\nabla_{\\theta} f_{\\theta}(z) \\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term of the last equation is not guaranteed to be an expectation, and we do not have an analytic solution to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea to solve this proposed by the paper is to have the stochastic element not as part of the model, but as its input. This can be achieved if instead of sampling $z \\sim q_\\phi(z \\mid x)$, $z$ can be rewritten  as a deterministic transformation of a noise variable $\\epsilon$ sampled from a standard Gaussian distribution $N(0, I)$. For a latent variable $z \\sim N(\\mu, \\sigma^2)$, this can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z = g_{\\phi}(x, \\epsilon), \\quad \\epsilon \\sim N(0, I)\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "g_{\\phi}(x, \\epsilon) = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we try the problematic differentiation again using $l \\in L$ to denote the lth Monte Carlo sample we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "E_{p_{\\theta}(z)} [f(z^{(i)})] &= E_{p(\\epsilon)} [f(g_{\\theta}(\\epsilon, x^{(i)}))] \\\\\n",
    "\\nabla_{\\theta} E_{p_{\\theta}(z)} [f(z^{(i)})] &= \\nabla_{\\theta} E_{p(\\epsilon)} [f(g_{\\theta}(\\epsilon, x^{(i)}))] \\\\\n",
    "&= E_{p(\\epsilon)} \\left[ \\nabla_{\\theta} f(g_{\\theta}(\\epsilon, x^{(i)})) \\right] \\\\\n",
    "&\\approx \\frac{1}{L} \\sum_{l=1}^{L} \\nabla_{\\theta} f(g_{\\theta}(\\epsilon^{(l)}, x^{(i)}))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the reparameterization trick we can again express a gradient of an expectation as an expectation of a gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./data/vae.png\" alt=\"Image\" width=\"1000>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"./data/vae.png\" alt=\"Image\" width=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closed Form of KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under some assumptions the KL divergence term has a closed form, namely when:\n",
    "* The prior $p(z)$ over the latent variables $z$ is a standard Gaussian $\\mathcal{N}(0, I)$, i.e., $p(z) = \\mathcal{N}(z|0, I)$.\n",
    "\n",
    "* The variational posterior $ q(z|x)$ is also a Gaussian, typically with a diagonal covariance matrix, i.e., $q(z|x) = \\mathcal{N}(z|\\mu(x), \\sigma(x)^2)$, where $\\mu(x) $ and $\\sigma(x)^2 $ are the outputs of the encoder network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under these conditions the closed form is :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{KL}(q(z|x) \\| p(z)) &= \\int q(z|x) \\log \\frac{q(z|x)}{p(z)} dz \\\\\n",
    "&= \\int \\mathcal{N}(z|\\mu(x), \\sigma(x)^2) \\log \\frac{\\mathcal{N}(z|\\mu(x), \\sigma(x)^2)}{\\mathcal{N}(z|0, I)} dz \\\\\n",
    "&= \\int \\mathcal{N}(z|\\mu(x), \\sigma(x)^2) \\left( \\log \\mathcal{N}(z|\\mu(x), \\sigma(x)^2) - \\log \\mathcal{N}(z|0, I) \\right) dz \\\\\n",
    "&= \\int \\mathcal{N}(z|\\mu(x), \\sigma(x)^2) \\left[ -\\frac{1}{2} \\left( \\log 2\\pi + \\log \\det(\\sigma(x)^2) + (z - \\mu(x))^T \\sigma(x)^{-2} (z - \\mu(x)) \\right) + \\frac{1}{2} \\left( (z^T z) \\right) \\right] dz \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is continuous theoretical form of the KL divergence for two Gaussian distributions. To be able to compute it we will need to discretize it to the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{KL}(q(z|x) \\| p(z)) \n",
    "&= \\frac{1}{2} \\left( \\text{tr}(\\sigma(x)^{-2}) + \\mu(x)^2 - d - \\log \\det(\\sigma(x)^2) \\right)\\\\\n",
    "&= \\frac{1}{2} \\left( \\mu(x)^2 + \\sigma(x)^2 - \\log \\sigma(x)^2 - 1 \\right)\\\\\n",
    "&= -\\frac{1}{2} \\left( 1  + \\log \\sigma(x)^2 - \\mu(x)^2 - \\sigma(x)^2  \\right)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring the ELBO in more detail we arrived at this expression:\n",
    "$$\n",
    "\\underset{\\text{Reconstruction term}}{\\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[ \\log p_{\\theta}(x \\mid z) \\right]} \n",
    "\n",
    "\\underset{\\text{KL Divergence term}}{- D_{\\text{KL}}(q_{\\phi}(z \\mid x) \\parallel p(z))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we want to maximize to have a good generative model. However, in machine learning we usually use gradient descent to find a minimum for the loss function of the model. To stick with this approach we will be minimizing the negative ELBO. With this in mind we can the define the VAE loss functions as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\theta, \\phi; x) = - \\mathbb{E}_{q_{\\phi}(z \\mid x)} \\left[ \\log p_{\\theta}(x \\mid z) \\right] + D_{\\text{KL}}(q_{\\phi}(z \\mid x) \\parallel p(z))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we outline the forward pass of the model:\n",
    "1. Push the input $x$ through the encoder:\n",
    "    * The input $x$ is passed through the encoder network, which outputs the mean $\\mu(x)$ and the standard deviation $\\sigma(x)$ (or equivalently the log-variance)\n",
    "    * $\\mu(x), \\sigma(x) = Encoder(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sample noise:\n",
    "    * To allow backpropagation through the stochastic part of the network, we sample $\\epsilon$ from a standard normal distribution $\\epsilon \\sim N(0,I)$\n",
    "    * $\\mu(x), \\sigma(x) = Endoder(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Reparametarize:\n",
    "    * Using the reparameterization trick, the latent variable $z$ is sampled as:\n",
    "    $$\n",
    "    g_{\\phi}(x, \\epsilon) = \\mu_{\\phi}(x) + \\sigma_{\\phi}(x) \\odot \\epsilon\n",
    "    $$\n",
    "    * This allows the gradient to flow through $\\mu(x)$ and  $\\sigma(x)$ during backpropagation\n",
    "    * In case  of using the log variance  $\\log(\\sigma^2)$ the formula for the standard deviation $ \\sigma $ can be derived in this way\n",
    "\n",
    "    -  Start with the logarithm of the variance:  \n",
    "   $$ \\text{logvar} = \\log(\\sigma^2) $$\n",
    "\n",
    "    -  Exponentiate both sides to obtain the variance:  \n",
    "   $$ \\sigma^2 = \\exp(\\text{logvar}) $$\n",
    "\n",
    "    -  Take the square root to get the standard deviation:  \n",
    "   $$ \\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\exp(\\text{logvar})} $$\n",
    "\n",
    "    -  Use the property of exponents $\\sqrt{\\exp(a)} = \\exp(a / 2) $:  \n",
    "   $$ \\sigma = \\exp(0.5 \\cdot \\text{logvar}) $$\n",
    "\n",
    "    -  Thus, the standard deviation is computed as:  \n",
    "   $$ \\sigma = \\exp(0.5 \\cdot \\text{logvar}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Push $z$ through the decoder:\n",
    "    * The reparameterized latent variable $z$ is then passed through the decoder to reconstruct the input $x$, yielding a predicted $\\bar{x}$\n",
    "    * $\\bar{x} = Decoder(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compute the reconstruction loss\n",
    "    * The reconstruction loss measures how well the decoder reconstructs the input, often using Mean Square Error (MSE), but in some specific cases as with black and white images we can Binary Cross Entropy (BCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Compute the variational loss (KL divergence)\n",
    "    * The variational loss measures the divergence between the approximate posterior $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu(x), \\sigma(x)^2)$ and the prior $p(z) = \\mathcal{N}(0, I)$\n",
    "    $$\\text{Var. Loss} = D_{\\text{KL}}[\\mathcal{N}(\\mu(x), \\sigma(x)^2) \\parallel \\mathcal{N}(0, I)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Combine the two loss terms for the final loss\n",
    "    * The total loss for the VAE is the combination of the reconstruction loss and the variational loss:\n",
    "    $$Total Loss=Reconstruction Loss + Variational Loss$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Autoencoder is a powerful concept on its own. It is a neural network that learns data representations in an unsupervised manner, without relying on labels. The network learns to compress data into a latent space in a way that allows it to be reconstructed as closely as possible to the original input. However, since it is trained using only reconstruction loss, there are no constraints placed on the structure of the latent space. This becomes problematic when we want to generate new, unseen samples from the latent space using only the decoder. In this case, the latent space lacks any inherent structure, which could result in irregularities like holes or discontinuities. If we try to generate a sample from a randomly chosen latent vector, there is no guarantee that it will correspond to a valid point in the data distribution, making the generation of realistic samples unreliable.\n",
    "\n",
    "The Variational Autoencoder (VAE) addresses this issue by adding a new term to the objective function: the KL divergence term. This term acts as a regularizer, effectively pushing points in the latent space closer to the origin. By doing so, it eliminates gaps in the latent space and often enforces a semantic structure, where similar inputs are mapped to nearby points in the latent space. The VAE's objective function can be seen as a delicate balance: the reconstruction term attempts to expand the latent space, while the variational term seeks to compact it. In the vanilla VAE implementation, these two terms are simply added together to form the objective function. However, later improvements introduce parameters that weight the terms, providing finer control over what the model is optimizing. This approach allows us to seek a fine-tuned balance between reconstruction quality and latent space regularization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
