{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resume of \"Auto-Encoding Variational Bayes\" (Kingma and Welling, 2014)**\n",
    "\n",
    "### **Introduction**\n",
    "The paper \"Auto-Encoding Variational Bayes\" introduces the Variational Autoencoder (VAE), a generative model that addresses the problem of efficient approximate inference in probabilistic models with latent variables.\n",
    "\n",
    "#### **Problem Statement**\n",
    "Latent variable models are powerful tools for unsupervised learning, where the goal is to infer latent (hidden) variables \\( z \\) that explain the observed data \\( x \\). The key challenges in using such models are:\n",
    "1. Computing the posterior distribution \\( p(z|x) \\), which is often intractable due to the complexity of \\( p(x) \\), the marginal likelihood of the data.\n",
    "2. Efficiently learning model parameters through optimization.\n",
    "\n",
    "#### **Proposed Solution**\n",
    "The authors propose using **variational inference**, a framework for approximating intractable posterior distributions, combined with an efficient reparameterization trick to enable gradient-based optimization. This approach allows for training both the generative model and the approximate posterior simultaneously. The resulting model, the VAE, combines probabilistic modeling with neural networks, making it scalable and efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### **Background**\n",
    "The VAE builds on the framework of probabilistic latent variable models. In these models:\n",
    "\n",
    "1. **Latent Variables:** A latent variable \\( z \\) is sampled from a prior distribution \\( p(z) \\).\n",
    "2. **Generative Process:** The observed data \\( x \\) is generated from a conditional likelihood \\( p(x|z) \\), parameterized by the latent variable \\( z \\).\n",
    "\n",
    "The marginal likelihood of the data is given by:\n",
    "\\[\n",
    "p(x) = \\int p(x|z)p(z)dz.\n",
    "\\]\n",
    "\n",
    "The posterior distribution \\( p(z|x) \\) is needed for inference but is generally intractable because computing \\( p(x) \\) requires integrating over \\( z \\). The authors approximate \\( p(z|x) \\) with a variational distribution \\( q(z|x) \\), parameterized by a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "### **Methodology**\n",
    "\n",
    "#### **Evidence Lower Bound (ELBO)**\n",
    "The key idea is to maximize a lower bound on the marginal likelihood \\( \\log p(x) \\), known as the Evidence Lower Bound (ELBO):\n",
    "\\[\n",
    "\\log p(x) \\geq \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z)),\n",
    "\\]\n",
    "where:\n",
    "- The first term, \\( \\mathbb{E}_{q(z|x)}[\\log p(x|z)] \\), is the expected log-likelihood of the data under the approximate posterior, also called the **reconstruction term**.\n",
    "- The second term, \\( D_{KL}(q(z|x) \\| p(z)) \\), is the Kullback-Leibler (KL) divergence between the approximate posterior \\( q(z|x) \\) and the prior \\( p(z) \\), which acts as a regularizer.\n",
    "\n",
    "The ELBO can be written as:\n",
    "\\[\n",
    "\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q(z|x; \\phi)}[\\log p(x|z; \\theta)] - D_{KL}(q(z|x; \\phi) \\| p(z)).\n",
    "\\]\n",
    "Here, \\( \\theta \\) are the parameters of the generative model \\( p(x|z) \\), and \\( \\phi \\) are the parameters of the variational distribution \\( q(z|x) \\).\n",
    "\n",
    "#### **Optimization of the ELBO**\n",
    "The authors propose to optimize the ELBO with respect to both \\( \\theta \\) and \\( \\phi \\). However, two challenges arise:\n",
    "1. **Reparameterization Trick:** To compute gradients with respect to \\( \\phi \\), the authors introduce the reparameterization trick. Instead of sampling \\( z \\sim q(z|x; \\phi) \\), they sample from a simple distribution (e.g., a standard normal) and transform the sample deterministically:\n",
    "   \\[\n",
    "   z = \\mu(x; \\phi) + \\sigma(x; \\phi) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I).\n",
    "   \\]\n",
    "   Here, \\( \\mu(x; \\phi) \\) and \\( \\sigma(x; \\phi) \\) are outputs of the encoder network, and \\( \\odot \\) denotes element-wise multiplication.\n",
    "\n",
    "2. **Stochastic Gradient Descent:** The reparameterization allows the gradient of the ELBO to be estimated efficiently using Monte Carlo methods, enabling the use of standard gradient-based optimizers.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Architecture**\n",
    "The VAE consists of two main components:\n",
    "\n",
    "1. **Encoder (Inference Model):** Approximates the posterior distribution \\( q(z|x; \\phi) \\). This is implemented as a neural network that outputs the mean \\( \\mu \\) and variance \\( \\sigma^2 \\) of a Gaussian distribution.\n",
    "   \\[\n",
    "   q(z|x; \\phi) = \\mathcal{N}(z; \\mu(x; \\phi), \\text{diag}(\\sigma^2(x; \\phi))).\n",
    "   \\]\n",
    "\n",
    "2. **Decoder (Generative Model):** Models the likelihood \\( p(x|z; \\theta) \\). Given a latent variable \\( z \\), the decoder generates data \\( x \\) through a neural network parameterized by \\( \\theta \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Derivation of the ELBO**\n",
    "To derive the ELBO, start with the marginal likelihood:\n",
    "\\[\n",
    "\\log p(x) = \\log \\int p(x|z)p(z)dz.\n",
    "\\]\n",
    "Introducing the variational distribution \\( q(z|x) \\):\n",
    "\\[\n",
    "\\log p(x) = \\log \\int q(z|x) \\frac{p(x|z)p(z)}{q(z|x)} dz.\n",
    "\\]\n",
    "Applying Jensen's inequality:\n",
    "\\[\n",
    "\\log p(x) \\geq \\int q(z|x) \\log \\frac{p(x|z)p(z)}{q(z|x)} dz = \\mathcal{L}(x; \\theta, \\phi).\n",
    "\\]\n",
    "The ELBO consists of two terms:\n",
    "1. **Reconstruction Loss:**\n",
    "   \\[\n",
    "   \\mathbb{E}_{q(z|x)}[\\log p(x|z)].\n",
    "   \\]\n",
    "2. **KL Divergence:**\n",
    "   \\[\n",
    "   -D_{KL}(q(z|x) \\| p(z)).\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Algorithm**\n",
    "The training process involves:\n",
    "1. Sampling a minibatch of data points \\( x \\).\n",
    "2. Passing \\( x \\) through the encoder to compute \\( \\mu(x; \\phi) \\) and \\( \\sigma(x; \\phi) \\).\n",
    "3. Sampling \\( z \\) using the reparameterization trick.\n",
    "4. Passing \\( z \\) through the decoder to compute \\( \\log p(x|z; \\theta) \\).\n",
    "5. Computing the KL divergence term.\n",
    "6. Optimizing the ELBO using backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Experiments**\n",
    "The authors evaluate the VAE on several benchmark datasets, including MNIST and Frey Faces, showing:\n",
    "- High-quality reconstructions and generative samples.\n",
    "- Robustness to overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Contributions**\n",
    "1. **Reparameterization Trick:** A novel way to compute gradients for stochastic latent variables, making variational inference scalable.\n",
    "2. **VAE Framework:** A combination of probabilistic modeling and deep learning for efficient and scalable unsupervised learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "The Variational Autoencoder (VAE) introduces a principled and practical framework for generative modeling, leveraging variational inference and neural networks. It combines probabilistic latent variable models with scalable optimization techniques, laying the groundwork for further advancements in deep generative models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
